{"cells":[{"cell_type":"markdown","source":["__1. Gazetteer Data__\n\n__*a. Create Unmanaged Tables*__\n\nThe first step of this assignment involves loading the data from the CSV files, combining the file with the file for the other year, and saving it to disk as a table. The following code should provide a template to help you combine tables and save them to the warehouse directory. Click on the image to download the sample code.\n\nFor each CSV file in the 2017 and 2018 directories, load the data into Spark, combine it with the corresponding data from the other year and save it to disk. Once you have finished saving all of the files as tables, verify that you have loaded the files properly by loading the tables into Spark, and performing a simple row count on each table.\n\nThe following Python code should provide you a template for loading the tables as an external table in Spark. Click on the image to download the sample code.\n\nAs stated previously, in a typical Hadoop distribution, you could save these tables as persistent tables in Apache Hive, but since we are not introducing Hive in this class, we need to load these tables into Spark and query them using SQL within Python.\n\nThe following code shows how to count the number of rows in the places table and show the results. Click on the image to download the code.\n\nAs an aside, spark.catalog module offers useful utility functions such as spark.catalog.listTables() to list all of the currently available tables. These are useful for inspecting the Spark SQL warehouse.\n\n__*b. Load and Query Tables*__\n\nNow that we have saved the data to external tables, we will load the tables back into Spark and create a report using Spark SQL. For this report, we will create a report on school districts for the states of Nebraska and Iowa using the elementary_schools, secondary_schools and unified_school_districts tables. Using Spark SQL, create a report with the following information.\n\nThis table contains the number of elementary, secondary, and unified school districts in each state for each year. Note that the numbers in this table are notional and do not represent the actual results."],"metadata":{}},{"cell_type":"code","source":["# Create necessary directories\nwarehouse_dir = \"/FileStore/spark-warehouse\"\ndbutils.fs.mkdirs(warehouse_dir) \ndbutils.fs.mkdirs(\"/FileStore/2017\") \ndbutils.fs.mkdirs(\"/FileStore/2018\") \n\ndisplay(dbutils.fs.ls(\"/FileStore\"))"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>path</th><th>name</th><th>size</th></tr></thead><tbody><tr><td>dbfs:/FileStore/2017/</td><td>2017/</td><td>0</td></tr><tr><td>dbfs:/FileStore/2018/</td><td>2018/</td><td>0</td></tr><tr><td>dbfs:/FileStore/df/</td><td>df/</td><td>0</td></tr><tr><td>dbfs:/FileStore/import-stage/</td><td>import-stage/</td><td>0</td></tr><tr><td>dbfs:/FileStore/sortDF.csv/</td><td>sortDF.csv/</td><td>0</td></tr><tr><td>dbfs:/FileStore/spark-warehouse/</td><td>spark-warehouse/</td><td>0</td></tr><tr><td>dbfs:/FileStore/tables/</td><td>tables/</td><td>0</td></tr></tbody></table></div>"]}}],"execution_count":2},{"cell_type":"markdown","source":["After this, I manually uploaded the `places.csv` files from 2017 and 2018, following the steps below:\n\n1. Upload the 2017 file to dbfs\n\n2. Move the 2017 file to 2017 directory\n\n  _dbutils.fs.cp(\"/FileStore/tables/places.csv\", \"/FileStore/2017/places.csv\")_\n\n3. Remove the 2017 file from dbfs\n\n  _dbutils.fs.rm(\"/FileStore/tables/places.csv\")_\n  \n4. Move the 2018 file to 2018 directory\n\n  _dbutils.fs.cp(\"/FileStore/tables/places.csv\", \"/FileStore/2018/places.csv\")_\n\n5. Remove the 2018 file from dbfs\n\n  _dbutils.fs.rm(\"/FileStore/tables/places.csv\")_\n  \nI had to do it this way since it was the only known method to me at the time of this excercise."],"metadata":{}},{"cell_type":"code","source":["from pyspark.sql import SparkSession\n\n# Create Spark session\nspark = SparkSession\\\n  .builder\\\n  .appName('DSC650Assignment5')\\\n  .config(\"spark.sql.warehouse.dir\", warehouse_dir)\\\n  .getOrCreate()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":4},{"cell_type":"code","source":["# Define data file path\ncsv_file_path2017 = '/FileStore/2017/places.csv'\ncsv_file_path2018 = '/FileStore/2018/places.csv'\n\n# Load data files\ndf2017 = spark.read.load(\n  csv_file_path2017,\n  format='csv',\n  sep=',',\n  inferSchema=True,\n  header=True\n)\n\ndf2018 = spark.read.load(\n  csv_file_path2018,\n  format='csv',\n  sep=',',\n  inferSchema=True,\n  header=True\n)\n\n# Check data volume for both files\nprint('2017 file row count:', df2017.count())\nprint('2018 file row count:', df2018.count())"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">2017 file row count: 29577\n2018 file row count: 29574\n</div>"]}}],"execution_count":5},{"cell_type":"code","source":["# Union the 2 data files\nplaces = df2017.unionAll(df2018)\n\n# Check total row count\nprint('Total row count:', places.count())\n\n# Save data to table in default database\nplaces.write.saveAsTable('places')"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"><span class=\"ansired\">---------------------------------------------------------------------------</span>\n<span class=\"ansired\">AnalysisException</span>                         Traceback (most recent call last)\n<span class=\"ansigreen\">&lt;command-3983604114395494&gt;</span> in <span class=\"ansicyan\">&lt;module&gt;</span><span class=\"ansiblue\">()</span>\n<span class=\"ansigreen\">      6</span> <span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">      7</span> <span class=\"ansired\"># Save data to table in default database</span><span class=\"ansiyellow\"></span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">----&gt; 8</span><span class=\"ansiyellow\"> </span>places<span class=\"ansiyellow\">.</span>write<span class=\"ansiyellow\">.</span>saveAsTable<span class=\"ansiyellow\">(</span><span class=\"ansiblue\">&apos;places&apos;</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n\n<span class=\"ansigreen\">/databricks/spark/python/pyspark/sql/readwriter.py</span> in <span class=\"ansicyan\">saveAsTable</span><span class=\"ansiblue\">(self, name, format, mode, partitionBy, **options)</span>\n<span class=\"ansigreen\">    775</span>         <span class=\"ansigreen\">if</span> format <span class=\"ansigreen\">is</span> <span class=\"ansigreen\">not</span> <span class=\"ansigreen\">None</span><span class=\"ansiyellow\">:</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    776</span>             self<span class=\"ansiyellow\">.</span>format<span class=\"ansiyellow\">(</span>format<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">--&gt; 777</span><span class=\"ansiyellow\">         </span>self<span class=\"ansiyellow\">.</span>_jwrite<span class=\"ansiyellow\">.</span>saveAsTable<span class=\"ansiyellow\">(</span>name<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    778</span> <span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    779</span>     <span class=\"ansiyellow\">@</span>since<span class=\"ansiyellow\">(</span><span class=\"ansicyan\">1.4</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n\n<span class=\"ansigreen\">/databricks/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py</span> in <span class=\"ansicyan\">__call__</span><span class=\"ansiblue\">(self, *args)</span>\n<span class=\"ansigreen\">   1255</span>         answer <span class=\"ansiyellow\">=</span> self<span class=\"ansiyellow\">.</span>gateway_client<span class=\"ansiyellow\">.</span>send_command<span class=\"ansiyellow\">(</span>command<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">   1256</span>         return_value = get_return_value(\n<span class=\"ansigreen\">-&gt; 1257</span><span class=\"ansiyellow\">             answer, self.gateway_client, self.target_id, self.name)\n</span><span class=\"ansigreen\">   1258</span> <span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">   1259</span>         <span class=\"ansigreen\">for</span> temp_arg <span class=\"ansigreen\">in</span> temp_args<span class=\"ansiyellow\">:</span><span class=\"ansiyellow\"></span>\n\n<span class=\"ansigreen\">/databricks/spark/python/pyspark/sql/utils.py</span> in <span class=\"ansicyan\">deco</span><span class=\"ansiblue\">(*a, **kw)</span>\n<span class=\"ansigreen\">     67</span>                                              e.java_exception.getStackTrace()))\n<span class=\"ansigreen\">     68</span>             <span class=\"ansigreen\">if</span> s<span class=\"ansiyellow\">.</span>startswith<span class=\"ansiyellow\">(</span><span class=\"ansiblue\">&apos;org.apache.spark.sql.AnalysisException: &apos;</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">:</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">---&gt; 69</span><span class=\"ansiyellow\">                 </span><span class=\"ansigreen\">raise</span> AnalysisException<span class=\"ansiyellow\">(</span>s<span class=\"ansiyellow\">.</span>split<span class=\"ansiyellow\">(</span><span class=\"ansiblue\">&apos;: &apos;</span><span class=\"ansiyellow\">,</span> <span class=\"ansicyan\">1</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">[</span><span class=\"ansicyan\">1</span><span class=\"ansiyellow\">]</span><span class=\"ansiyellow\">,</span> stackTrace<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">     70</span>             <span class=\"ansigreen\">if</span> s<span class=\"ansiyellow\">.</span>startswith<span class=\"ansiyellow\">(</span><span class=\"ansiblue\">&apos;org.apache.spark.sql.catalyst.analysis&apos;</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">:</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">     71</span>                 <span class=\"ansigreen\">raise</span> AnalysisException<span class=\"ansiyellow\">(</span>s<span class=\"ansiyellow\">.</span>split<span class=\"ansiyellow\">(</span><span class=\"ansiblue\">&apos;: &apos;</span><span class=\"ansiyellow\">,</span> <span class=\"ansicyan\">1</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">[</span><span class=\"ansicyan\">1</span><span class=\"ansiyellow\">]</span><span class=\"ansiyellow\">,</span> stackTrace<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n\n<span class=\"ansired\">AnalysisException</span>: &apos;Table &#96;places&#96; already exists.;&apos;</div>"]}}],"execution_count":6},{"cell_type":"code","source":["# I am going so save the dataframes as external table in the warehouse\ndf2017.write.option(\"path\", warehouse_dir).saveAsTable(\"df2017\", mode='overwrite')\n\ndf2018.write.option(\"path\", warehouse_dir).saveAsTable(\"df2018\", mode='overwrite')\n\nplaces.write.option(\"path\", warehouse_dir).saveAsTable(\"places\", mode='overwrite')"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":7},{"cell_type":"code","source":["# Query the data using the sqlContext method\nsqlContext.sql(\"SELECT COUNT(*) FROM places\").show()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+--------+\ncount(1)|\n+--------+\n   59151|\n+--------+\n\n</div>"]}}],"execution_count":8},{"cell_type":"code","source":["# Query the data using the spark.sql method - a preferred one for Spark 2x\nspark.sql(\"SELECT COUNT(*) FROM places\").show()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+--------+\ncount(1)|\n+--------+\n   21779|\n+--------+\n\n</div>"]}}],"execution_count":9},{"cell_type":"code","source":["# load school district data for 2017 and 2018\n# Define data file path\npath2017 = '/FileStore/2017/'\npath2018 = '/FileStore/2018/'\n\nelementary_file_2017 = path2017+'elementary_schools.csv'\nsecondary_file_2017 = path2017+'secondary_schools.csv'\nunified_file_2017 = path2017+'unified_school_districts.csv'\n\nelementary_file_2018 = path2018+'elementary_schools.csv'\nsecondary_file_2018 = path2018+'secondary_schools.csv'\nunified_file_2018 = path2018+'unified_school_districts.csv'\n\n# Load data files - 2017\n# Elementary\nelementary2017 = spark.read.load(\n  elementary_file_2017,\n  format='csv',\n  sep=',',\n  inferSchema=True,\n  header=True\n)\n\n# Secondary\nsecondary2017 = spark.read.load(\n  secondary_file_2017,\n  format='csv',\n  sep=',',\n  inferSchema=True,\n  header=True\n)\n\n# Unified school district\nunified2017 = spark.read.load(\n  unified_file_2017,\n  format='csv',\n  sep=',',\n  inferSchema=True,\n  header=True\n)\n\n# Load data files - 2018\n# Elementary\nelementary2018 = spark.read.load(\n  elementary_file_2018,\n  format='csv',\n  sep=',',\n  inferSchema=True,\n  header=True\n)\n\n# Secondary\nsecondary2018 = spark.read.load(\n  secondary_file_2018,\n  format='csv',\n  sep=',',\n  inferSchema=True,\n  header=True\n)\n\n# Unified school district\nunified2018 = spark.read.load(\n  unified_file_2018,\n  format='csv',\n  sep=',',\n  inferSchema=True,\n  header=True\n)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":10},{"cell_type":"code","source":["# Union the 2 data files\nelementary = elementary2017.unionAll(elementary2018)\nsecondary = secondary2017.unionAll(secondary2018)\nunified = unified2017.unionAll(unified2018)\n\n# Check total row count\nprint('Total elementary row count:', elementary.count())\nprint('Total secondary row count:', secondary.count())\nprint('Total unified row count:', unified.count())\n\n# Save data to table in default database\nelementary.write.saveAsTable('elementary')\nsecondary.write.saveAsTable('secondary')\nunified.write.saveAsTable('unified')"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Total elementary row count: 3926\nTotal secondary row count: 974\nTotal unified row count: 21779\n</div>"]}}],"execution_count":11},{"cell_type":"code","source":["# I am going so save the dataframes as external table in the warehouse\nelementary.write.option(\"path\", warehouse_dir).saveAsTable(\"elementary\", mode='overwrite')\nsecondary.write.option(\"path\", warehouse_dir).saveAsTable(\"secondary\", mode='overwrite')\nunified.write.option(\"path\", warehouse_dir).saveAsTable(\"unified\", mode='overwrite')"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":12},{"cell_type":"markdown","source":["I encountered an errror while adding the secondary data. The data contain mixed data type in `water_area_meters_sq` field (integer and int64). I have researched and found some sort of explanation in this link: (https://stackoverflow.com/questions/50383360/parquet-datatype-issue).\n\nFor the time being, I have proceeded with only elementary and unified data."],"metadata":{}},{"cell_type":"code","source":["display(spark.sql(\"SELECT a.state as State, a.year as Year, a.cnt Elementary, b.cnt Unified \\\n                   FROM (SELECT state, year, count(*) as cnt \\\n                         FROM elementary \\\n                         WHERE state IN('NE','IA') \\\n                         GROUP BY state, year) a \\\n                   LEFT OUTER JOIN \\\n                        (SELECT state, year, count(*) as cnt \\\n                         FROM unified \\\n                         WHERE state IN('NE','IA') \\\n                         GROUP BY state, year) b \\\n                    ON a.state = b.state \\\n                    AND a.year = b.year \\\n                    ORDER BY a.state, a.year\"))"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>State</th><th>Year</th><th>Elementary</th><th>Unified</th></tr></thead><tbody><tr><td>IA</td><td>2017</td><td>336</td><td>336</td></tr><tr><td>IA</td><td>2018</td><td>333</td><td>333</td></tr><tr><td>NE</td><td>2017</td><td>251</td><td>251</td></tr><tr><td>NE</td><td>2018</td><td>246</td><td>246</td></tr></tbody></table></div>"]}}],"execution_count":14},{"cell_type":"code","source":[""],"metadata":{},"outputs":[],"execution_count":15}],"metadata":{"name":"5.2","notebookId":3983604114395486},"nbformat":4,"nbformat_minor":0}
