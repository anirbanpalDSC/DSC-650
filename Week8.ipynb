{"cells":[{"cell_type":"markdown","source":["##Stream Structured Data\n\n*With some initial hickups, local Spark installation has been used.*\n\nIn the first part of the exercise, you will create a simple Spark streaming program that reads an input stream from a file source. The file source stream reader reads data from a directory on a file system. When a new file is added to the folder, Spark adds that fileâ€™s data to the input data stream.\n\nYou can find the input data for this exercise in the baby-names/streaming directory. This directory contains the baby names CSV file randomized and split into 98 individual files. You will use these files to simulate incoming streaming data.\n\n### a. Count the Number of Females\n\nIn the first part of the exercise, you will create a Spark program that monitors an incoming directory. To simulate streaming data, you will copy CSV files from the baby-names/streaming directory into the incoming directory. Since you will be loading CSV data, you will need to define a schema before you initialize the streaming dataframe.\n\nFrom this input data stream, you will create a simple output data stream that counts the number of females and writes it to the console. Approximately every 10 seconds or so, copy a new file into the directory and report the console output. Do this for the first ten files."],"metadata":{}},{"cell_type":"code","source":["# import libraries\nimport os.path \nimport shutil\nimport glob\nimport os\nimport os.path \n\nfrom pyspark.sql import SparkSession\nfrom pyspark import SparkConf\nfrom pyspark import SparkContext\nfrom time import sleep\nfrom pyspark.sql.functions import window\n\n# create sprk context\nsc = SparkContext.getOrCreate(SparkConf().setMaster(\"local[*]\"))"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":2},{"cell_type":"code","source":["# define directories\nfile_path = r'/home/dsc650-master/data/baby-names/streaming'\nipstreaming_file_path = r'/home/dsc650-master/input_streaming'\nbatchstream_file_path = r'/home/dsc650-master/batch_streaming'"],"metadata":{},"outputs":[],"execution_count":3},{"cell_type":"code","source":["# define the schema from the statis file\nspark = SparkSession.builder.appName('strstream').getOrCreate()\nstatic = spark.read.csv(file_path, header = True)\ndataschema = static.schema\n\n# check structure\nstatic.printSchema()"],"metadata":{},"outputs":[],"execution_count":4},{"cell_type":"code","source":["# reading stream from input directory\nstreaming = spark.readStream.schema(dataschema).csv(ipstreaming_file_path) \n\n# check count\ncounts = streaming.groupBy(\"sex\").count()\ncounts"],"metadata":{},"outputs":[],"execution_count":5},{"cell_type":"code","source":["# stop stream\nstreamingquery.stop()"],"metadata":{},"outputs":[],"execution_count":6},{"cell_type":"code","source":["streamingquery = counts.writeStream.queryName(\"counts\").format(\"memory\").outputMode(\"complete\").start()"],"metadata":{},"outputs":[],"execution_count":7},{"cell_type":"code","source":["# print active streams\nspark.streams.active"],"metadata":{},"outputs":[],"execution_count":8},{"cell_type":"code","source":["print(static.isStreaming)"],"metadata":{},"outputs":[],"execution_count":9},{"cell_type":"code","source":["# display stream progress\nfor x in range(10):\n    spark.sql(\"SELECT * FROM counts\").show()\n    sleep(1)"],"metadata":{},"outputs":[],"execution_count":10},{"cell_type":"code","source":["fileslist = os.listdir(file_path)\nprint(fileslist[1:11])\n\n# getting list of files in the directory\nfiles_list = glob.glob(\"/home/dsc650-master/data/baby-names/streaming/*.csv\")\nprint(files_list)"],"metadata":{},"outputs":[],"execution_count":11},{"cell_type":"code","source":["# display sourec and destination path\nprint(os.path.basename(file_path))\nprint(os.path.dirname(files_list[1]))\nprint(os.path.split(files_list[1]) )\n\nfilename = os.path.splitext(files_list[1])[0]\nprint(filename)\n\nprint(os.path.join('input_streaming', os.path.dirname(files_list[1])))\n\nsrc_path = os.path.join(file_path,fileslist[4])\n\ndest_path = os.path.join(ipstreaming_file_path,fileslist[4])\n\nprint(src_path, dest_path)"],"metadata":{},"outputs":[],"execution_count":12},{"cell_type":"code","source":["# display counts\nfor i in range(len(fileslist[1:11])):\n    \n    file = fileslist[i]\n    \n    print(file)\n    src_path = os.path.join(file_path,file)\n    dest_path = os.path.join(ipstreaming_file_path,file)\n    \n    shutil.copy(src_path, dest_path) \n    print(\"File moved \\n\")\n    \n    \n    print(\"Check counts \\n \")\n    sleep(2)\n    spark.sql(\"SELECT * FROM counts\").show()\n    sleep(2)"],"metadata":{},"outputs":[],"execution_count":13},{"cell_type":"markdown","source":["### 2. Micro-Batching\n\nRepeat the last step, but use a micro-batch interval to trigger the processing every 30 seconds. Approximately every 10 seconds or so, copy a new file into the directory and report the console output. Do this for the first ten files. How did the output differ from the previous example?"],"metadata":{}},{"cell_type":"code","source":["# import package\nfrom pyspark.streaming import StreamingContext\n\n# define streaming context\nstc = Streamingcontext()"],"metadata":{},"outputs":[],"execution_count":15},{"cell_type":"code","source":["# transform and read stream from input directory\ncsvmb = spark.readStream.schema(dataschema).csv(batchstream_file_path)  \nbatch_counts = csvmb.groupBy(\"sex\").count()\n\n# get count of females\nbatch_counts.select(\"sex\").where(\"sex = 'F'\")\nbatch_counts.groupby(\"sex\").count()"],"metadata":{},"outputs":[],"execution_count":16},{"cell_type":"code","source":["# define bacth writer\nmicrobatch_writer = batch_counts.\\\n  writeStream.\\\n  trigger(processingTime = '30 seconds').\\\n  queryName(\"batch_counts\").\\\n  format(\"memory\").\\\n  outputMode(\"complete\").\n  start()"],"metadata":{},"outputs":[],"execution_count":17},{"cell_type":"code","source":["# start batch writer\nmicrobatch_writer.isActive"],"metadata":{},"outputs":[],"execution_count":18},{"cell_type":"code","source":["# display micro batch result\nfor i in range(len(fileslist[1:11])):\n    \n    file = fileslist[i]\n    \n    print(file)\n    src_path = os.path.join(file_path,file)\n    dest_path = os.path.join(batchstream_file_path,file)\n    \n    shutil.copy(src_path, dest_path) \n    print(\"File moved \\n\")\n    \n    \n    print(\"Check the counts \\n \")\n    sleep(1)\n    spark.sql(\"SELECT * FROM batch_counts\").show()\n    sleep(10)"],"metadata":{},"outputs":[],"execution_count":19}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"mimetype":"text/x-python","name":"python","pygments_lexer":"ipython3","codemirror_mode":{"name":"ipython","version":3},"version":"3.7.3","nbconvert_exporter":"python","file_extension":".py"},"name":"Copy of Week8-Copy1","notebookId":3917026457204149},"nbformat":4,"nbformat_minor":0}
